{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8224a8df-e982-43ac-a5cf-5bee409ba593",
   "metadata": {},
   "source": [
    "# File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5032b2dc-7e15-4636-a901-577d97a6214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510e52d-86c8-4590-bc29-0bbe87c21c12",
   "metadata": {},
   "source": [
    "Computers have finite resources. You have likely encountered these limits firsthand if your computer has slowed down from having too many applications open at once. We want to make sure that we do not exceed the computer’s limits while working with data, and we might examine a file differently, depending on its size. If we know that our dataset is relatively small, then a text editor or a spreadsheet can be convenient to look at the data. On the other hand, for large datasets, a more programmatic exploration or even distributed computing tools may be needed.\n",
    "\n",
    "In many situations, we analyze datasets downloaded from the Internet. These files reside on the computer’s disk storage. In order to use Python to explore and manipulate the data, we need to read the data into the computer’s memory, also known as random access memory (RAM). All Python code requires the use of RAM, no matter how short the code is.\n",
    "\n",
    "A computer’s RAM is typically much smaller than a computer’s disk storage. For example, one computer model released in 2018 had 32 times more disk storage than RAM. Unfortunately, this means that data files can often be much bigger than what is feasible to read into memory. Both disk storage and RAM capacity are measured in terms of bytes. Roughly speaking, each character in a text file adds one byte to a file’s size. To succinctly describe the sizes of larger files, we use the prefixes as described in the following Table 8.1."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd3ce679-6395-481f-8fd5-f0a21598aa48",
   "metadata": {},
   "source": [
    "#{table} Prefixes for common filesizes.\n",
    "#    :name: byte-prefixes\n",
    "    | Multiple | Notation | Number of Bytes |\n",
    "    | -------- | -------- | --------------- |\n",
    "    | Kibibyte | KiB      | 1024    |\n",
    "    | Mebibyte | MiB      | 1024²   |\n",
    "    | Gibibyte | GiB      | 1024³   |\n",
    "    | Tebibyte | TiB      | 1024⁴   |\n",
    "    | Pebibyte | PiB      | 1024⁵   |\n",
    "    :::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f147b-cffa-4dd2-b6a9-b77688cacc9b",
   "metadata": {},
   "source": [
    "For example, a file containing 52428800 characters takes up 52428800/10242=50 mebibytes, or 50 MiB on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4e369-a1d9-428f-8e09-769098004c16",
   "metadata": {},
   "source": [
    "Many computers have much more disk storage than available memory. It is not uncommon to have a data file happily stored on a computer that will overflow the computer’s memory if we attempt to manipulate it with a program, including Python programs. We often begin our data work by making sure the files we are of manageable size. To accomplish this, we can use the built-in os library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78c0d130-9b53-4c4f-baf3-cafe88da460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File                      Size(KiB)\n",
      "data/inspections.csv      455.0\n",
      "data/nyt_names.csv        0.0\n",
      "data/.DS_Store            6.0\n",
      "data/babynames.csv        33345.0\n",
      "data/babynames.db         42288.0\n",
      "data/DAWN-Data.txt        273531.0\n",
      "data/businesses.csv       645.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "kib = 1024\n",
    "line = '{:<25} {}'.format\n",
    "\n",
    "print(line('File','Size(KiB)'))\n",
    "for filepath in Path('data').glob('*'):\n",
    "    size = os.path.getsize(filepath)\n",
    "    print(line(str(filepath), np.round(size/kib)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad307325-0707-467b-b943-882b58bb4494",
   "metadata": {},
   "source": [
    "We see that the businesses.csv file takes up 645 KiB on disk, making it well within the memory capacities of most systems. Although the violations.csv file takes up 3.6 MiB of disk storage, most machines can easily read it into a Pandas DataFrame too. But DAWN-Data.txt, which contains the DAWN survey data, is much larger.\n",
    "\n",
    "The DAWN file takes up nearly 270 MiB of disk storage, and while some computers can work with this file in memory, it can slow down other systems. To make this data more manageable in Python, we can load in a subset of the columns rather than all of them.\n",
    "\n",
    "Sometimes we are interested in the total size of a folder instead of the size of individual files. For example, if we have one file of inspections for each month in a year, we might like to see whether we can combine all the data into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c2eee2-4ce0-464f-ab0d-8f8c8d2009fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data/ folder contains 342.06 MiB\n"
     ]
    }
   ],
   "source": [
    "mib = 1024**2\n",
    "\n",
    "total = 0\n",
    "for filepath in Path('data').glob('*'):\n",
    "    total+=os.path.getsize(filepath)/mib\n",
    "    \n",
    "print(f'The data/ folder contains {total:.2f} MiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05405092-6700-4e2c-8656-0c51d8d16731",
   "metadata": {},
   "source": [
    "A computer with 4 GiB total RAM might have only 1 GiB available RAM with many applications running. With 1 GiB available RAM, it is unlikely that pandas will be able to read in a 1 GiB file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edddb152-8379-4171-ae65-3078f30d3aac",
   "metadata": {},
   "source": [
    "## Working with large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdac89a-58e9-46ee-8e51-7f31961272f2",
   "metadata": {},
   "source": [
    "we discuss strategies for working with data that are far larger than what is feasible to load into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87c31d-cf8f-4845-badf-869fa62bf35e",
   "metadata": {},
   "source": [
    "### Subset The Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc257dc3-dc70-4cb8-815c-63561d7fe3e8",
   "metadata": {},
   "source": [
    "One simple approach is to subset the data. Rather than loading in the entire source file, we can either select a specific part of it (e.g. one day’s worth of data), or we can randomly sample the dataset. Because of its simplicity, we use this approach quite often in this book. The natural downside is that this approach loses many of the benefits of analyzing a large dataset, like being able to study rare events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a2ca2-097b-49af-a4de-fa0bfac5f909",
   "metadata": {},
   "source": [
    "### Use a Database System."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47deafdc-9d28-4e24-9f50-65f324fe4c59",
   "metadata": {},
   "source": [
    "As we discussed in Chapter 7, relational database management systems (RDBMS) were specifically designed to store large datasets. These systems can manipulate data that are too large to fit into memory using SQL queries. Because of their advantages, RDBMS’s are common in research and industry settings for data storage. One downside is that they often require a separate server for the data that needs its own configuration. Another downside is that SQL is less flexible in what it can compute than Python, which becomes especially relevant for modeling and prediction. One useful hybrid approach is to use SQL to subset, aggregate, or sample the data into batches that are small enough to read into Python. Then, we can use Python for more sophisticated analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea4bbcb-8b41-4735-b482-2f9237f67b75",
   "metadata": {},
   "source": [
    "### Use a Distributed Computing System."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5ad5f-67cf-4202-bae1-d21732d453cd",
   "metadata": {},
   "source": [
    "Another approach to handle complex computations on large datasets is to use a distributed computing system like MapReduce, Spark, or Ray. These systems work best on tasks that can be split into many smaller parts since they split up large datasets into smaller ones, then run programs on all of the smaller datasets at once. Because of this, these systems have great flexibility and can be used in a variety of scenarios like modeling and prediction. Their main downside is that they can require a lot of work to install and configure properly, since these systems are typically installed across many computers that need to coordinate with each other.\n",
    "\n",
    "In sum, this section introduced common file size notation and showed how to check file sizes in Python. It is convenient to use Python to determine a file format, encoding, and size. Another powerful tool for working with files is the shell, which is widely used and has a more succinct syntax than Python. In the next section, we introduce a few command-line tools available in the shell for carrying out the same tasks of finding out information about a file before reading it into a data frame."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
